{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f12855",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.initializers import GlorotUniform, GlorotNormal, HeUniform, HeNormal, Zeros, Ones\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import linregress\n",
    "from joblib import Parallel, delayed\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d432fc",
   "metadata": {},
   "source": [
    "### Real-Time Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d09c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set of function to read and process the raw spectra\n",
    "def read_files(loc):\n",
    "    files_all = sorted(os.listdir(loc), key = len)\n",
    "    ABS_names = []\n",
    "    PL_names = []\n",
    "    ABS_all = []\n",
    "    PL_all = []\n",
    "    for file in files_all:\n",
    "        if (file.startswith(\"Abs\")):\n",
    "            ABS = pd.read_csv(loc + \"//\" + file , header = None).to_numpy()\n",
    "            ABS_all.append(ABS)\n",
    "            ABS_names.append(file.split(\".\")[0])\n",
    "        elif (file.startswith(\"PL\")):\n",
    "            PL = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "            PL_all.append(PL)\n",
    "            PL_names.append(file.split(\".\")[0])\n",
    "        elif(file.startswith(\"Wavelength_Abs\")):\n",
    "            WL_ABS = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"Wavelength_PL\")):\n",
    "            WL_PL = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"DR_Abs\")):\n",
    "            DR_ABS = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"DR_PL\")):\n",
    "            DR_PL = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"LR\")):\n",
    "            LR = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"FR\")):\n",
    "            FR = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()    \n",
    "    return WL_ABS, WL_PL, DR_ABS, DR_PL, LR, ABS_all, PL_all, ABS_names, PL_names, FR\n",
    "def idx_min(y, x):\n",
    "    diff = np.abs(y[:, 0] - x)\n",
    "    idx = diff.argmin()\n",
    "    return idx\n",
    "# extract reactive phase\n",
    "def extract(x, idx1, idx2, numFirstElements):\n",
    "    x_mean = x[idx1:idx2 + 1, :].mean(axis = 0)\n",
    "    x_sort = np.sort(x_mean)\n",
    "    idx_phase = np.nonzero(np.in1d(x_mean, x_sort[:numFirstElements]))[0]\n",
    "    x_phase = x[:, idx_phase]\n",
    "    return x_phase\n",
    "def spectra_avg(x):\n",
    "    x_avg = x.mean(axis = 1) # avg over extracted reactive phase spectra for each WL; will result in a row matrix\n",
    "    x_avg = x_avg[:, np.newaxis]\n",
    "    return x_avg\n",
    "def baseline_zero(x, idx_low, idx_high):\n",
    "    x_baseline_zero = x - x[idx_low:idx_high + 1, :].mean() # make baseline zero; subtracting mean of PL at LL - HL nm\n",
    "    return x_baseline_zero\n",
    "def linear_int_x(y, x, i, y_btw):\n",
    "    x_btw = x[i, 0] - ((x[i + 1, 0] - x[i, 0]) / (y[i + 1, 0] - y[i, 0])) * (y[i, 0] - y_btw)\n",
    "    return x_btw\n",
    "def linear_int_y(y, x, i, x_btw):\n",
    "    y_btw = y[i, 0] - ((y[i + 1, 0] - y[i, 0]) / (x[i + 1, 0] - x[i, 0])) * (x[i, 0] - x_btw)\n",
    "    return y_btw\n",
    "def peak_info(y, x, emission_intensity):\n",
    "    # correction for emission peak intensity in case min is not exactly zero\n",
    "    # min_intensity = y.min()\n",
    "    min_intensity = 0 # without correction\n",
    "    intensity_peak = emission_intensity - min_intensity\n",
    "    # emission peak area\n",
    "    area_peak = np.trapz(y.T, x = x.T)[0]\n",
    "    # emission peak intensity and area\n",
    "    output = [intensity_peak, area_peak]\n",
    "    return np.array(output)[:, np.newaxis]\n",
    "# process ABS\n",
    "def spectra_extract_ABS(WL, DR, LR, ABS_all, ABS_names):\n",
    "    # baseline WL limits\n",
    "    baseline_LL = 700\n",
    "    baseline_HL = 800\n",
    "    idx_WL_LL = idx_min(WL, baseline_LL)\n",
    "    idx_WL_HL = idx_min(WL, baseline_HL)\n",
    "    # to extract reactive phase\n",
    "    lastPeak_LL = 250\n",
    "    lastPeak_HL = 350 \n",
    "    idx_lastPeak_LL = idx_min(WL, lastPeak_LL)\n",
    "    idx_lastPeak_HL = idx_min(WL, lastPeak_HL)\n",
    "    # excitation WL info\n",
    "    excitation_WL = 300\n",
    "    idx_excitationWL = idx_min(WL, excitation_WL)\n",
    "    # initiate files to be exported\n",
    "    WLABS_processed = WL\n",
    "    ABS_excitationWL_list = []\n",
    "    # ABS_excitationWL_list = np.array([[\"Absorbance at excitation WL\"]])\n",
    "    for (item, name) in zip(ABS_all, ABS_names):\n",
    "        ABS_only = item[1:, :]\n",
    "        ## extract reactive phase and remove carrier phase (PFO)\n",
    "        param = 50\n",
    "        ABS_phase = extract(ABS_only, idx_lastPeak_LL, idx_lastPeak_HL, param)\n",
    "        ABS_phase_avg = spectra_avg(ABS_phase) \n",
    "        arg_log = (ABS_phase_avg - DR) / (LR - DR) # Beer-Lambert law\n",
    "        ABS_processed = - np.log10(arg_log) # Beer-Lambert law\n",
    "        ABS_processed_baseline = baseline_zero(ABS_processed, idx_WL_LL, idx_WL_HL) \n",
    "        WLABS_processed = np.concatenate( [WLABS_processed, ABS_processed_baseline] , axis = 1) # attach WL and processed ABS\n",
    "        # WLABS_processed_filtered = WLABS_processed[np.isnan(WLABS_processed[:, 1]) == False] # remove NaN ABS\n",
    "        ABS_excitationWL = linear_int_y(ABS_processed_baseline, WL, idx_excitationWL, excitation_WL)\n",
    "        ABS_excitationWL_list.append(ABS_excitationWL)\n",
    "        # ABS_excitationWL_list = np.concatenate( [ABS_excitationWL_list, np.array([ABS_excitationWL])[:, np.newaxis]] , axis = 1)\n",
    "    ABS_excitationWL_list = np.array(ABS_excitationWL_list)[:, np.newaxis]\n",
    "    return ABS_excitationWL_list\n",
    "# process PL\n",
    "def spectra_extract_PL(WL, DR, PL_all, PL_names):\n",
    "    ## baseline WL limits\n",
    "    baseline_LL = 800\n",
    "    baseline_HL = 1000\n",
    "    idx_WL_LL = idx_min(WL, baseline_LL)\n",
    "    idx_WL_HL = idx_min(WL, baseline_HL)\n",
    "    # to extract reactive phase\n",
    "    excWL_LL = 250\n",
    "    excWL_HL = 350\n",
    "    idx_excWL_LL = idx_min(WL, excWL_LL)\n",
    "    idx_excWL_HL = idx_min(WL, excWL_HL)\n",
    "    ## emission peak WL and the WL range for the right side of emission peak\n",
    "    emPeak_WL = 446\n",
    "    emPeak_LL = 446\n",
    "    emPeak_HL = 700\n",
    "    idx_emPeak_LL = idx_min(WL, emPeak_LL)\n",
    "    idx_emPeak_HL = idx_min(WL, emPeak_HL)\n",
    "    WL_aroundPeak = WL[idx_emPeak_LL: idx_emPeak_HL + 1, :]\n",
    "    idx_intensityWL = idx_min(WL, emPeak_WL) # index for emission peak WL\n",
    "    ## initiate files to be exported\n",
    "    WLPL_processed = WL\n",
    "    emission_details_list = []\n",
    "    # emission_details = np.array([[\"Emission Peak WL\"], [\"Emission Peak Intensity\"], [\"FWHM\"], [\"Emission Peak Area\"]])\n",
    "    for (item, name) in zip(PL_all, PL_names):\n",
    "        PL_only = item[1:, :]\n",
    "        ## extract reactive phase and remove carrier phase (PFO)\n",
    "        param = 50\n",
    "        PL_phase = extract(PL_only, idx_excWL_LL, idx_excWL_HL, param) \n",
    "        PL_phase_avg = spectra_avg(PL_phase)\n",
    "        PL_processed = PL_phase_avg - DR\n",
    "        PL_processed_baseline = baseline_zero(PL_processed, idx_WL_LL, idx_WL_HL) \n",
    "        WLPL_processed = np.concatenate( [WLPL_processed, PL_processed_baseline] , axis = 1) # attach WL and processed PL\n",
    "        ## emission peak info\n",
    "        PL_emissionWL = linear_int_y(PL_processed_baseline, WL, idx_intensityWL, emPeak_WL)\n",
    "        PL_aroundPeak = PL_processed_baseline[idx_emPeak_LL: idx_emPeak_HL + 1, :]\n",
    "        info_peak = peak_info(PL_aroundPeak, WL_aroundPeak, PL_emissionWL)\n",
    "        emission_details_list.append(info_peak)\n",
    "        # emission_details = np.concatenate( [emission_details, info_peak_method] , axis = 1)\n",
    "    emission_details_list = np.concatenate(emission_details_list, axis = 1).T\n",
    "    emission_details_list[:,[1]] = emission_details_list[:,[1]] * 2 # since we calculated the area under the curve for the half right side of the emission peak\n",
    "    return emission_details_list[:, [0]] # emission peak intensity as the single output of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fe09b",
   "metadata": {},
   "source": [
    "### ML Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale output\n",
    "def non_dim_y(y_initial):\n",
    "    y_scaled = y_initial / 5000\n",
    "    return y_scaled\n",
    "## nondimensionalize inputs\n",
    "def non_dim_x(x_initial):\n",
    "    x_normalized = np.zeros((x_initial.shape[0], x_initial.shape[1] - 4))\n",
    "    x_normalized[:, 0] = (x_initial[:, 0] - 120) / (150 - 120) # x1 = temperature\n",
    "    x_normalized[:, 1] = (x_initial[:, 1] - 40) / (90 - 40) # x2 = CuI\n",
    "    x_normalized[:, 2] = (x_initial[:, 2] - 15) / (30 - 15) # x3 = OA-CuI\n",
    "    x_normalized[:, 3] = (x_initial[:, 3] - 15) / (30 - 15) # x4 =  OAm-CuI\n",
    "    x_normalized[:, 4] = (x_initial[:, 4] - 80) / (100 - 80) # x5 = ODE-CuI\n",
    "    x_normalized[:, 5] = (x_initial[:, 5] - 40) / (90 - 40) # x6 = CsOleate\n",
    "    x_normalized[:, 6] = (x_initial[:, 6] - 15) / (30 - 15) # x7 = OA-CsOleate\n",
    "    return x_normalized\n",
    "## split dataset to training and validation\n",
    "def split_set(x, y, train_ratio = 0.8, seed_num = 100):\n",
    "    xy_all = np.concatenate((x, y), axis = 1)\n",
    "    np.random.seed(seed_num)  \n",
    "    np.random.shuffle(xy_all)\n",
    "    # training set\n",
    "    xy_train = xy_all[:int(train_ratio * xy_all.shape[0]), :]\n",
    "    x_train = xy_train[:, :-y.shape[1]]\n",
    "    y_train = xy_train[:, -y.shape[1]:]\n",
    "    # validation set\n",
    "    xy_test = xy_all[int(train_ratio * xy_all.shape[0]):, :]\n",
    "    x_test = xy_test[:, :-y.shape[1]]\n",
    "    y_test = xy_test[:, -y.shape[1]:]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "## ENN structure\n",
    "def structure_info(num_models, layer_min = 6, layer_max = 8, node_min = 15, node_max = 30, seed_num = 100):\n",
    "    random.seed(seed_num)\n",
    "    detail_all = []\n",
    "    for i in range (num_models):\n",
    "        num_layers = random.randint(layer_min, layer_max)\n",
    "        nodes_list = []\n",
    "        for j in range(num_layers):\n",
    "            num_nodes = random.randint(node_min, node_max)\n",
    "            nodes_list.append(num_nodes)\n",
    "        detail = [num_layers, nodes_list]\n",
    "        detail_all.append(detail)\n",
    "    return detail_all\n",
    "## build and train an ensemble of cascade NNs\n",
    "def parallel_train_ML(train_x, train_y, val_x, val_y, structure):\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    ## cascade NN\n",
    "    input = Input(shape = train_x.shape[1])\n",
    "    x = input\n",
    "    for layer in range(structure[0]):\n",
    "        output = Dense(structure[1][layer], activation = 'relu', kernel_initializer = HeUniform(seed = 42), bias_initializer = Zeros())(x)\n",
    "        x = Concatenate()([x, output])\n",
    "    # dropout = layers.Dropout(0.1)(x)\n",
    "    # output = Dense(train_y.shape[1], activation = 'linear')(dropout)\n",
    "    output = Dense(train_y.shape[1], activation = 'linear')(x)\n",
    "    model = Model(inputs = input, outputs = output)\n",
    "    ## train the model\n",
    "    # early stopping terminates training if validation model loss begins to increase due to over-fitting\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 50, min_delta = 1e-5, mode = 'min', restore_best_weights = True)\n",
    "    # early_stopping = EarlyStopping(monitor = 'val_loss', patience = 50, min_delta = 1e-5, mode = 'min')\n",
    "    # model compile with loss, optimizer, and performance metric of interest\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = RMSprop(learning_rate = 1e-4), metrics = [RootMeanSquaredError()])\n",
    "    # perform Model Training with training and validation data with early stopping\n",
    "    history = model.fit(train_x, train_y, validation_data = (val_x, val_y), epochs = 1000, verbose = 0, callbacks = [early_stopping])\n",
    "    result = [model, history]\n",
    "    return result\n",
    "## deconvolute the model and training history\n",
    "def deconvolute_ML(ensemble_result):\n",
    "    ensemble_model = []\n",
    "    ensemble_history = []\n",
    "    for element in ensemble_result:\n",
    "        ensemble_model.append(element[0])\n",
    "        ensemble_history.append(element[1])\n",
    "    return ensemble_model, ensemble_history\n",
    "## plots related to the training process\n",
    "def learn_plot(history):\n",
    "    fig = plt.figure()\n",
    "    # loss values\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], color = 'blue')\n",
    "    plt.plot(history.history['val_loss'], color = 'red')\n",
    "    plt.title('Loss (MSE) - Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    # accuracy values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['root_mean_squared_error'], color = 'blue')\n",
    "    plt.plot(history.history['val_root_mean_squared_error'], color = 'red')\n",
    "    plt.title('RMSE - Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "## plot related to the predicted vs. true outputs - for individual models    \n",
    "def predict_plot_separate(model, x, y_true):\n",
    "    y_true = np.squeeze(y_true)\n",
    "    predicted_y = model.predict(x, verbose = 0)\n",
    "    detail_1 = linregress(y_true, np.squeeze(predicted_y))\n",
    "    # detail_2 = linregress(y_true[:, 1], predicted_y[:, 1])\n",
    "    plt.scatter(y_true, np.squeeze(predicted_y), color = 'blue')\n",
    "    # plt.scatter(y_true[:, 1], predicted_y[:, 1], color = 'red')\n",
    "    plt.plot([0, 1],[0, 1], color = 'green')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    # plt.legend(['y0: $R^2 = $' + str(detail_1.rvalue ** 2), 'y1: $R^2 = $' + str(detail_2.rvalue ** 2), 'Parity'])\n",
    "    plt.legend(['y0: $R^2 = $' + str(detail_1.rvalue ** 2), 'Parity'])\n",
    "    plt.show()\n",
    "## plot related to the predicted vs. true outputs - for the entire ENN  \n",
    "def predict_plot_mean(ensemble_NN, x, y_true):\n",
    "    y_true = np.squeeze(y_true)\n",
    "    y_pred = []\n",
    "    for model in ensemble_NN:\n",
    "        y_pred.append(model.predict(x, verbose = 0))\n",
    "    y_pred_arr = np.concatenate(y_pred, axis = 1)\n",
    "    out_mean = np.mean(y_pred_arr, axis = 1)\n",
    "    out_std = np.std(y_pred_arr, axis = 1)\n",
    "    out_detail = linregress(y_true, out_mean)\n",
    "    plt.errorbar(y_true, out_mean, yerr = out_std, fmt = 'o', color = 'blue', alpha = 0.8, markersize = 5, label = 'y: $R^2 = $' + str(round((out_detail.rvalue ** 2), 4)))\n",
    "    plt.plot([0, 1],[0, 1], color = 'red', label = 'Parity')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    return out_mean, out_std\n",
    "## save the ENN in the given path\n",
    "def save_ENN(ENN, path):\n",
    "    for n, model in enumerate(ENN):\n",
    "        os.mkdir(path + \"//\" + str(n))\n",
    "        model.save(path + \"//\" + str(n))\n",
    "## load the ENN stored in the given path\n",
    "def load_plot(path):\n",
    "    model_idx = sorted(os.listdir(path), key = len)\n",
    "    ensemble = []\n",
    "    for idx in model_idx:\n",
    "        ensemble.append(load_model(path + \"//\" + idx))\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f8368",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used of expected improvement (EI)\n",
    "def best_target_data(y_norm, y_target):\n",
    "    obj_fun_data = abs(y_norm - y_target)\n",
    "    obj_fun_best = obj_fun_data.min()\n",
    "    return obj_fun_best\n",
    "## aquisition functions implementation\n",
    "def apply_policy(x_initial, ensemble, y_target, obj_fun_best, policy):\n",
    "    # p = psutil.Process()\n",
    "    # p.nice(psutil.REALTIME_PRIORITY_CLASS)    \n",
    "    x = x_initial[np.newaxis, :]\n",
    "    obj_models = []\n",
    "    for model in ensemble:\n",
    "        y_pred = model.predict(x, verbose = 0)\n",
    "        # our ptoblem is single objective optimization\n",
    "        obj_fun = abs(y_target - np.squeeze(y_pred)[()])\n",
    "        obj_models.append(obj_fun)\n",
    "    ## EI\n",
    "    if (policy[0] == \"EI\"):\n",
    "        imp_list = []\n",
    "        for element in obj_models:\n",
    "            diff = obj_fun_best - element \n",
    "            if (diff >= 0):\n",
    "                imp_list.append(diff)\n",
    "            else:\n",
    "                imp_list.append(0)\n",
    "        value = - np.mean(imp_list)\n",
    "    ## UCB\n",
    "    if (policy[0] == \"UCB\"):\n",
    "        value = np.mean(obj_models) - (policy[1] * np.std(obj_models))\n",
    "    ## MV      \n",
    "    if (policy[0] == \"MV\"):\n",
    "        value = - np.std(obj_models)\n",
    "    ## EPLT\n",
    "    elif (policy[0] == \"EPLT\"):\n",
    "        value =  np.mean(obj_models)\n",
    "    return value\n",
    "## optimization\n",
    "def opt_general(f, dim):\n",
    "    # p = psutil.Process()\n",
    "    # p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    low_limit = [0] * dim\n",
    "    up_limit = [1] * dim\n",
    "    bnds = list(zip(low_limit, up_limit))\n",
    "    count_success = 0\n",
    "    while (count_success < 1):\n",
    "        x_initial = np.random.uniform(0, 1, dim)\n",
    "        # sol = minimize(f, x_initial, method = \"trust-constr\", bounds = bnds, options = {'gtol': 0.02, 'xtol': 0.02, 'verbose': 0})\n",
    "        sol = minimize(f, x_initial, method = \"SLSQP\", bounds = bnds, options = {'ftol': 0.02, 'maxiter': 200})\n",
    "        if ((sol.success == True) and (sol.nit > 1)):\n",
    "            result = sol\n",
    "            count_success = count_success + 1\n",
    "    return result      \n",
    "def parallel_opt(ensemble, y_target, obj_fun_best, policy, x_shape):\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    model_pol = lambda x: apply_policy(x, ensemble, y_target, obj_fun_best, policy)\n",
    "    solution = opt_general(model_pol, x_shape)\n",
    "    return solution\n",
    "## use this while running multiple optimization; here we used this for MV explorative campaign\n",
    "# def deconvolute_opt(solution, dim, num_restart, num_selection, threshold_ratio = 0.3):\n",
    "#     xy = np.zeros((num_restart, dim + 1))\n",
    "#     for count in range(len(solution)):\n",
    "#         xy[count, :-1] = solution[count].x\n",
    "#         xy[count, -1] = solution[count].fun\n",
    "#     xy_sort = xy[xy[:, -1].argsort()]\n",
    "#     dist_arr = np.linalg.norm(xy_sort[1:, :-1] - xy_sort[[0], :-1], axis = 1)\n",
    "#     threshold = threshold_ratio * np.amax(dist_arr)\n",
    "#     final_xy = xy_sort[[0], :]\n",
    "#     for i in range(1, xy_sort.shape[0]):\n",
    "#         if np.all(np.linalg.norm(final_xy[:, :-1] - xy_sort[[i], :-1], axis = 1) > threshold):\n",
    "#             final_xy = np.vstack((final_xy, xy_sort[[i], :]))\n",
    "#     select_xy = final_xy[:num_selection, :]\n",
    "#     return select_xy\n",
    "## use this while running one single optimization; here we used this for EI, UCB, and EPLT optimization campaigns \n",
    "def deconvolute_opt(solution, dim, num_restart, num_selection):\n",
    "    xy = np.zeros((num_restart, dim + 1))\n",
    "    for count in range(len(solution)):\n",
    "        xy[count, :-1] = solution[count].x\n",
    "        xy[count, -1] = solution[count].fun\n",
    "    select_xy = xy[:num_selection, :]\n",
    "    return select_xy\n",
    "## use this while running multiple optimization; here we used this for MV explorative campaign  \n",
    "# def exp_sel(y, FR, y_target, policy, num_selection, num_models = 20, num_restart = 5):\n",
    "#     ## dataset and preprocessing needed to be able to train the model\n",
    "#     y_norm = non_dim_y(y)\n",
    "#     obj_fun_best = best_target_data(y_norm, y_target)\n",
    "#     # a = y.shape[0]\n",
    "#     # x_norm = non_dim_x(FR[:a, :])\n",
    "#     x_norm = non_dim_x(FR)\n",
    "#     x_train, y_train, x_valid, y_valid = split_set(x_norm, y_norm)\n",
    "#     ## train the ENN\n",
    "#     start_ML = time()\n",
    "# #     p = psutil.Process()\n",
    "# #     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "#     arthitecture = structure_info(num_models)\n",
    "#     result_aggregate_ML = Parallel(n_jobs = -2)(delayed(parallel_train_ML)(x_train, y_train, x_valid, y_valid, arthitecture[i]) for i in range(num_models))\n",
    "#     ensemble, history = deconvolute_ML(result_aggregate_ML)\n",
    "#     end_ML = time()\n",
    "#     print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "#     print(f\"The execution time to train the ensemble NN is {str(end_ML - start_ML)} seconds.\")\n",
    "#     print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "#     ## optimization and achive the best input variables needed for the future experiments\n",
    "#     start_opt = time()\n",
    "# #     p = psutil.Process()\n",
    "# #     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "#     result_aggregate_opt = Parallel(n_jobs = -2)(delayed(parallel_opt)(ensemble, y_target, obj_fun_best, policy, x_norm.shape[1]) for i in range(num_restart))\n",
    "#     xy_best = deconvolute_opt(result_aggregate_opt, x_norm.shape[1], num_restart, num_selection)\n",
    "#     # sort based on the temperature\n",
    "#     xy_best_sort = xy_best[xy_best[:, 0].argsort()]\n",
    "#     x_best = xy_best_sort[:, :-1]\n",
    "#     y_best = xy_best_sort[:, [-1]]\n",
    "#     end_opt = time()\n",
    "#     print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "#     print(f\"The execution time for optimization is {str(end_opt - start_opt)} seconds.\")\n",
    "#     print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "#     return x_best, y_best, result_aggregate_opt, ensemble, history, x_train, y_train, x_valid, y_valid, arthitecture\n",
    "## use this while running one single optimization; here we used this for EI, UCB, and EPLT optimization campaigns\n",
    "def exp_sel(y, FR, y_target, policy, num_selection, num_models = 20, num_restart = 1):\n",
    "    ## dataset and preprocessing needed to be able to train the model\n",
    "    y_norm = non_dim_y(y)\n",
    "    obj_fun_best = best_target_data(y_norm, y_target)\n",
    "    # a = y.shape[0]\n",
    "    # x_norm = non_dim_x(FR[:a, :])\n",
    "    x_norm = non_dim_x(FR)\n",
    "    x_train, y_train, x_valid, y_valid = split_set(x_norm, y_norm)\n",
    "    ## train the ENN\n",
    "    start_ML = time()\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    arthitecture = structure_info(num_models)\n",
    "    result_aggregate_ML = Parallel(n_jobs = -2)(delayed(parallel_train_ML)(x_train, y_train, x_valid, y_valid, arthitecture[i]) for i in range(num_models))\n",
    "    ensemble, history = deconvolute_ML(result_aggregate_ML)\n",
    "    end_ML = time()\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"The execution time to train the ensemble NN is {str(end_ML - start_ML)} seconds.\")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    ## optimization and achive the best input variables needed for the future experiments\n",
    "    start_opt = time()\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    result_aggregate_opt = Parallel(n_jobs = -2)(delayed(parallel_opt)(ensemble, y_target, obj_fun_best, policy, x_norm.shape[1]) for i in range(num_restart))\n",
    "    xy_best = deconvolute_opt(result_aggregate_opt, x_norm.shape[1], num_restart, num_selection)\n",
    "    # sort based on the temperature\n",
    "    x_best = xy_best[:, :-1]\n",
    "    y_best = xy_best[:, [-1]]\n",
    "    end_opt = time()\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"The execution time for optimization is {str(end_opt - start_opt)} seconds.\")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    return x_best, y_best, result_aggregate_opt, ensemble, history, x_train, y_train, x_valid, y_valid, arthitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b50df",
   "metadata": {},
   "source": [
    "### Master Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(path):\n",
    "    files_all = os.listdir(path)\n",
    "    count_ABS = 0\n",
    "    count_PL = 0\n",
    "    for file in files_all:\n",
    "        if (file.startswith(\"Abs\")):\n",
    "            count_ABS = count_ABS + 1\n",
    "        elif (file.startswith(\"PL\")):\n",
    "            count_PL = count_PL + 1\n",
    "    return count_ABS, count_PL\n",
    "def x_dim(x_norm, FR, path):\n",
    "    x_final = np.zeros((x_norm.shape[0], x_norm.shape[1] + 4))\n",
    "    x_final[:, 0] = (x_norm[:, 0] * (150 - 120)) + 120\n",
    "    x_final[:, 1] = (x_norm[:, 1] * (90 - 40)) + 40\n",
    "    x_final[:, 2] = (x_norm[:, 2] * (30 - 15)) + 15\n",
    "    x_final[:, 3] = (x_norm[:, 3] * (30 - 15)) + 15\n",
    "    x_final[:, 4] = (x_norm[:, 4] * (100 - 80)) + 80\n",
    "    x_final[:, 5] = (x_norm[:, 5] * (90 - 40)) + 40\n",
    "    x_final[:, 6] = (x_norm[:, 6] * (30 - 15)) + 15\n",
    "    # last four columns\n",
    "    x_final[:, 7] = np.sum(x_final[:, 1:5], axis = 1) - np.sum(x_final[:, 5:7], axis = 1) # x8 = ODE-CsOleate\n",
    "    x_final[:, -3] = np.sum(x_final[:, 1:8], axis = 1) # x9 = PFO\n",
    "    x_final[:, -2] = 2 # x10 = 2 always; wait for 2 residence times then record the spectra\n",
    "    # x_final[:, -1] = 0 # x11 = 0 always; no washing\n",
    "    updated_FR = np.concatenate((FR, x_final), axis = 0) ## concat previous and new input parameters\n",
    "    np.savetxt(path + \"//\" + \"FR.csv\", updated_FR, delimiter = \",\", fmt=\"%1.3f\") # overwrite it into the existing FR.csv file\n",
    "    return x_final, updated_FR\n",
    "## master function\n",
    "## use this while running multiple optimization and selecting 2 with an algorithm for 1 in que; here we used this for MV explorative campaign \n",
    "# def master_code(y_target = 1.0, policy = [\"MV\", 0], num_selection = 2, exp_budget = 61): # to test using the provided dataset\n",
    "# def master_code(y_target = 1.0, policy = [\"MV\", 0], num_selection = 2, exp_budget = 59):\n",
    "#     loc = input(\"Please enter the path for the directory that includes files: \")\n",
    "#     print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "#     num_files_old = 0\n",
    "#     while True:\n",
    "#         num_ABS, num_PL = count_files(loc)\n",
    "#         if (num_ABS == num_PL):\n",
    "#             num_files = num_PL\n",
    "#         if (num_files > num_files_old) and (num_files < exp_budget):\n",
    "#             num_files_old = num_files\n",
    "#             WL_ABS, WL_PL, DR_ABS, DR_PL, LR, ABS_all, PL_all, ABS_names, PL_names, FR = read_files(loc)\n",
    "#             if (num_files > FR.shape[0] - 2):\n",
    "#                 print(\"New run starts now:\")\n",
    "#                 y = spectra_extract_PL(WL_PL, DR_PL, PL_all, PL_names)\n",
    "#                 FR_used = FR[:num_files, :]\n",
    "#                 new_x, new_y, solution_all, ensemble, history, x_train, y_train, x_valid, y_valid, structure = exp_sel(y, FR_used, y_target, policy, num_selection)\n",
    "#                 new_FR, updated_FR = x_dim(new_x, FR, loc)\n",
    "#         elif (num_files >= exp_budget):\n",
    "#             print(\"The experimental budget is reached!\")\n",
    "#             break\n",
    "#         ## use this for the test run using the provided dataset \n",
    "#         # if (num_files_old == 60):\n",
    "#             # break\n",
    "#     # return new_x, new_y, solution_all, ensemble, history, x_train, y_train, x_valid, y_valid, structure, new_FR, updated_FR\n",
    "## master function\n",
    "## use this while running one single optimization; here we used this for EI, UCB, and EPLT optimization campaigns\n",
    "## policy can be [\"MV\", 0], [\"EPLT\", 0], [\"EI\", 0], or [\"UCB\", 1. / math.sqrt(2)]\n",
    "# def master_code(y_target = 1.0, policy = [\"UCB\", 1. / math.sqrt(2)], num_selection = 1, exp_budget = 61): # to test using the provided dataset\n",
    "# def master_code(y_target = 1.0, policy = [\"EPLT\", 0], num_selection = 1, exp_budget = 70): # if EPLT is used\n",
    "# def master_code(y_target = 1.0, policy = [\"UCB\", 1. / math.sqrt(2)], num_selection = 1, exp_budget = 70): # if UCB is used\n",
    "def master_code(y_target = 1.0, policy = [\"EI\", 0], num_selection = 1, exp_budget = 70): # if EI is used\n",
    "    loc = input(\"Please enter the path for the directory that includes files: \")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    num_files_old = 0\n",
    "    while True:\n",
    "        num_ABS, num_PL = count_files(loc)\n",
    "        if (num_ABS == num_PL):\n",
    "            num_files = num_PL\n",
    "        if (num_files > num_files_old + num_selection - 1) and (num_files < exp_budget):\n",
    "            print(\"New run starts now:\")\n",
    "            num_files_old = num_files\n",
    "            WL_ABS, WL_PL, DR_ABS, DR_PL, LR, ABS_all, PL_all, ABS_names, PL_names, FR = read_files(loc)\n",
    "            y = spectra_extract_PL(WL_PL, DR_PL, PL_all, PL_names)\n",
    "            new_x, new_y, solution_all, ensemble, history, x_train, y_train, x_valid, y_valid, structure = exp_sel(y, FR, y_target, policy, num_selection)\n",
    "            new_FR, updated_FR = x_dim(new_x, FR, loc)\n",
    "        elif (num_files >= exp_budget):\n",
    "            print(\"The experimental budget is reached!\")\n",
    "            break\n",
    "        ## use this for the test run using the provided dataset \n",
    "        # if (num_files_old == 60):\n",
    "            # break\n",
    "    # return new_x, new_y, solution_all, ensemble, history, x_train, y_train, x_valid, y_valid, structure, new_FR, updated_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85383c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_x, new_y, solution_all, ensemble, history, x_train, y_train, x_valid, y_valid, structure, new_FR, updated_FR = master_code()\n",
    "master_code()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
